{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1857db3a-6bd1-46f8-8c45-971dbfdb5a92",
   "metadata": {},
   "source": [
    "# SWin Transformer from scratch with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b258cf-44b4-4203-8343-83a2a7300ff2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Importing Depenedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42c7f8a0-e833-432d-a582-6b2817eedb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /home/muhamedessam/anaconda3/lib/python3.9/site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4059308e-c4a3-46f0-bd48-6160c30c6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26212815-471b-4e5d-be9b-3f7dab8d4fc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Model Building\n",
    "1. Different SWin Configurations\n",
    "2. Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29396432-c4c1-48c7-977a-a9dd12a60230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "\n",
    "def swin_s(hidden_dim=96, layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "\n",
    "def swin_b(hidden_dim=128, layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "\n",
    "def swin_l(hidden_dim=192, layers=(2, 2, 18, 2), heads=(6, 12, 24, 48), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16fbbbc6-35dd-4e50-b376-69f309b723e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
    "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
    "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
    "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
    "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
    "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim * 8),\n",
    "            nn.Linear(hidden_dim * 8, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.stage1(img)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9379f-93f0-4c96-8b06-a205225150dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Building Blocks\n",
    "1. Stage Module.\n",
    "2. Patch Merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84e934d5-789c-47ce-84e4-c9ce63f19e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
    "                 relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
    "\n",
    "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
    "                                            downscaling_factor=downscaling_factor)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_partition(x)\n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            x = regular_block(x)\n",
    "            x = shifted_block(x)\n",
    "        return x.permute(0, 3, 1, 2) # B , C , H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8d86c19-4893-4581-bbdf-fecd572fe6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e8db35a-2b22-44e6-81c4-d972687ad2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Define input tensor\n",
    "x = torch.randn(1, 3, 8, 8)  # batch size=1, channels=3, height=8, width=8\n",
    "\n",
    "downscaling_factor=2\n",
    "# # Define Unfold module\n",
    "unfold = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
    "\n",
    "# # Apply Unfold to input tensor\n",
    "out = unfold(x).view(1, -1, 2,2)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ea396-92d1-46f9-9cd2-514bc86df9b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Swin Block #1\n",
    "1. Swin Block.\n",
    "2. Layer Norm.\n",
    "3. Residual.\n",
    "4. Feed Forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64c25a0f-f484-49bc-a086-de2f80a45517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
    "                                                                     heads=heads,\n",
    "                                                                     head_dim=head_dim,\n",
    "                                                                     shifted=shifted,\n",
    "                                                                     window_size=window_size,\n",
    "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        return x\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef484a01-0315-4230-80f8-d389baab26ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Swin Block #2\n",
    "1. Cyclic Shift.\n",
    "2. Create Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb01c2cf-b5d5-450b-ac3b-ff5c64bf3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "441ac64c-97b1-431c-aff6-fb7cdee7249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:\n",
      " tensor([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n",
      "        [ 11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.],\n",
      "        [ 21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.],\n",
      "        [ 31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.],\n",
      "        [ 41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.],\n",
      "        [ 51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.],\n",
      "        [ 61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.],\n",
      "        [ 71.,  72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.],\n",
      "        [ 81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.],\n",
      "        [ 91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99., 100.]])\n",
      "Cyclic:\n",
      " tensor([[ 12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  11.],\n",
      "        [ 22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  21.],\n",
      "        [ 32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  31.],\n",
      "        [ 42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,  41.],\n",
      "        [ 52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.,  51.],\n",
      "        [ 62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  61.],\n",
      "        [ 72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  71.],\n",
      "        [ 82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.,  81.],\n",
      "        [ 92.,  93.,  94.,  95.,  96.,  97.,  98.,  99., 100.,  91.],\n",
      "        [  2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,   1.]])\n",
      "Reversed Cyclic:\n",
      " tensor([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n",
      "        [ 11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.],\n",
      "        [ 21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.],\n",
      "        [ 31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.],\n",
      "        [ 41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.],\n",
      "        [ 51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.],\n",
      "        [ 61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.],\n",
      "        [ 71.,  72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.],\n",
      "        [ 81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.],\n",
      "        [ 91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99., 100.]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.linspace(1,100,100).reshape(10,10)\n",
    "print('BEFORE:\\n',x)\n",
    "y=torch.roll(x, shifts=(-1, -1),dims=(0,1))\n",
    "print('Cyclic:\\n',y)\n",
    "z=torch.roll(y, shifts=(1, 1),dims=(0,1))\n",
    "print('Reversed Cyclic:\\n',z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3623bab-7fff-4f5a-8781-3143eaf61400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "    if upper_lower:\n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34ab3c4b-5d2d-4c16-ab7e-e59678975fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape: torch.Size([16, 16])\n",
      "mask shape: torch.Size([4, 4, 4, 4])\n",
      "tensor([[[[0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf]],\n",
      "\n",
      "         [[0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf]],\n",
      "\n",
      "         [[-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.]],\n",
      "\n",
      "         [[-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf]],\n",
      "\n",
      "         [[0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf]],\n",
      "\n",
      "         [[-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.]],\n",
      "\n",
      "         [[-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf]],\n",
      "\n",
      "         [[0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf]],\n",
      "\n",
      "         [[-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.]],\n",
      "\n",
      "         [[-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf]],\n",
      "\n",
      "         [[0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., -inf, -inf]],\n",
      "\n",
      "         [[-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.]],\n",
      "\n",
      "         [[-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.],\n",
      "          [-inf, -inf, 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "window_size = 4\n",
    "displacement = 2\n",
    "upper_lower = False\n",
    "left_right = True\n",
    "\n",
    "# Create mask tensor\n",
    "mask = create_mask(window_size, displacement, upper_lower, left_right)\n",
    "\n",
    "print('mask shape:',mask.shape)\n",
    "# Reshape mask tensor for visualization\n",
    "mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "print('mask shape:',mask.shape)\n",
    "\n",
    "# Print the mask tensor\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f1475c-12ca-4954-9115-982c72b93eb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. SWin Block #3\n",
    "1. Relative Position Embedding\n",
    "2. Window Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2394dfb1-dcc5-4216-9654-0bb9f9be09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e068ae2-6ace-47fa-b536-70f77aba281d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2],\n",
       "         [ 0,  3],\n",
       "         [ 1,  0],\n",
       "         [ 1,  1],\n",
       "         [ 1,  2],\n",
       "         [ 1,  3],\n",
       "         [ 2,  0],\n",
       "         [ 2,  1],\n",
       "         [ 2,  2],\n",
       "         [ 2,  3],\n",
       "         [ 3,  0],\n",
       "         [ 3,  1],\n",
       "         [ 3,  2],\n",
       "         [ 3,  3]],\n",
       "\n",
       "        [[ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2],\n",
       "         [ 1, -1],\n",
       "         [ 1,  0],\n",
       "         [ 1,  1],\n",
       "         [ 1,  2],\n",
       "         [ 2, -1],\n",
       "         [ 2,  0],\n",
       "         [ 2,  1],\n",
       "         [ 2,  2],\n",
       "         [ 3, -1],\n",
       "         [ 3,  0],\n",
       "         [ 3,  1],\n",
       "         [ 3,  2]],\n",
       "\n",
       "        [[ 0, -2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 1, -2],\n",
       "         [ 1, -1],\n",
       "         [ 1,  0],\n",
       "         [ 1,  1],\n",
       "         [ 2, -2],\n",
       "         [ 2, -1],\n",
       "         [ 2,  0],\n",
       "         [ 2,  1],\n",
       "         [ 3, -2],\n",
       "         [ 3, -1],\n",
       "         [ 3,  0],\n",
       "         [ 3,  1]],\n",
       "\n",
       "        [[ 0, -3],\n",
       "         [ 0, -2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 1, -3],\n",
       "         [ 1, -2],\n",
       "         [ 1, -1],\n",
       "         [ 1,  0],\n",
       "         [ 2, -3],\n",
       "         [ 2, -2],\n",
       "         [ 2, -1],\n",
       "         [ 2,  0],\n",
       "         [ 3, -3],\n",
       "         [ 3, -2],\n",
       "         [ 3, -1],\n",
       "         [ 3,  0]],\n",
       "\n",
       "        [[-1,  0],\n",
       "         [-1,  1],\n",
       "         [-1,  2],\n",
       "         [-1,  3],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2],\n",
       "         [ 0,  3],\n",
       "         [ 1,  0],\n",
       "         [ 1,  1],\n",
       "         [ 1,  2],\n",
       "         [ 1,  3],\n",
       "         [ 2,  0],\n",
       "         [ 2,  1],\n",
       "         [ 2,  2],\n",
       "         [ 2,  3]],\n",
       "\n",
       "        [[-1, -1],\n",
       "         [-1,  0],\n",
       "         [-1,  1],\n",
       "         [-1,  2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2],\n",
       "         [ 1, -1],\n",
       "         [ 1,  0],\n",
       "         [ 1,  1],\n",
       "         [ 1,  2],\n",
       "         [ 2, -1],\n",
       "         [ 2,  0],\n",
       "         [ 2,  1],\n",
       "         [ 2,  2]],\n",
       "\n",
       "        [[-1, -2],\n",
       "         [-1, -1],\n",
       "         [-1,  0],\n",
       "         [-1,  1],\n",
       "         [ 0, -2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 1, -2],\n",
       "         [ 1, -1],\n",
       "         [ 1,  0],\n",
       "         [ 1,  1],\n",
       "         [ 2, -2],\n",
       "         [ 2, -1],\n",
       "         [ 2,  0],\n",
       "         [ 2,  1]],\n",
       "\n",
       "        [[-1, -3],\n",
       "         [-1, -2],\n",
       "         [-1, -1],\n",
       "         [-1,  0],\n",
       "         [ 0, -3],\n",
       "         [ 0, -2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 1, -3],\n",
       "         [ 1, -2],\n",
       "         [ 1, -1],\n",
       "         [ 1,  0],\n",
       "         [ 2, -3],\n",
       "         [ 2, -2],\n",
       "         [ 2, -1],\n",
       "         [ 2,  0]],\n",
       "\n",
       "        [[-2,  0],\n",
       "         [-2,  1],\n",
       "         [-2,  2],\n",
       "         [-2,  3],\n",
       "         [-1,  0],\n",
       "         [-1,  1],\n",
       "         [-1,  2],\n",
       "         [-1,  3],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2],\n",
       "         [ 0,  3],\n",
       "         [ 1,  0],\n",
       "         [ 1,  1],\n",
       "         [ 1,  2],\n",
       "         [ 1,  3]],\n",
       "\n",
       "        [[-2, -1],\n",
       "         [-2,  0],\n",
       "         [-2,  1],\n",
       "         [-2,  2],\n",
       "         [-1, -1],\n",
       "         [-1,  0],\n",
       "         [-1,  1],\n",
       "         [-1,  2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2],\n",
       "         [ 1, -1],\n",
       "         [ 1,  0],\n",
       "         [ 1,  1],\n",
       "         [ 1,  2]],\n",
       "\n",
       "        [[-2, -2],\n",
       "         [-2, -1],\n",
       "         [-2,  0],\n",
       "         [-2,  1],\n",
       "         [-1, -2],\n",
       "         [-1, -1],\n",
       "         [-1,  0],\n",
       "         [-1,  1],\n",
       "         [ 0, -2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 1, -2],\n",
       "         [ 1, -1],\n",
       "         [ 1,  0],\n",
       "         [ 1,  1]],\n",
       "\n",
       "        [[-2, -3],\n",
       "         [-2, -2],\n",
       "         [-2, -1],\n",
       "         [-2,  0],\n",
       "         [-1, -3],\n",
       "         [-1, -2],\n",
       "         [-1, -1],\n",
       "         [-1,  0],\n",
       "         [ 0, -3],\n",
       "         [ 0, -2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 1, -3],\n",
       "         [ 1, -2],\n",
       "         [ 1, -1],\n",
       "         [ 1,  0]],\n",
       "\n",
       "        [[-3,  0],\n",
       "         [-3,  1],\n",
       "         [-3,  2],\n",
       "         [-3,  3],\n",
       "         [-2,  0],\n",
       "         [-2,  1],\n",
       "         [-2,  2],\n",
       "         [-2,  3],\n",
       "         [-1,  0],\n",
       "         [-1,  1],\n",
       "         [-1,  2],\n",
       "         [-1,  3],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2],\n",
       "         [ 0,  3]],\n",
       "\n",
       "        [[-3, -1],\n",
       "         [-3,  0],\n",
       "         [-3,  1],\n",
       "         [-3,  2],\n",
       "         [-2, -1],\n",
       "         [-2,  0],\n",
       "         [-2,  1],\n",
       "         [-2,  2],\n",
       "         [-1, -1],\n",
       "         [-1,  0],\n",
       "         [-1,  1],\n",
       "         [-1,  2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2]],\n",
       "\n",
       "        [[-3, -2],\n",
       "         [-3, -1],\n",
       "         [-3,  0],\n",
       "         [-3,  1],\n",
       "         [-2, -2],\n",
       "         [-2, -1],\n",
       "         [-2,  0],\n",
       "         [-2,  1],\n",
       "         [-1, -2],\n",
       "         [-1, -1],\n",
       "         [-1,  0],\n",
       "         [-1,  1],\n",
       "         [ 0, -2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0],\n",
       "         [ 0,  1]],\n",
       "\n",
       "        [[-3, -3],\n",
       "         [-3, -2],\n",
       "         [-3, -1],\n",
       "         [-3,  0],\n",
       "         [-2, -3],\n",
       "         [-2, -2],\n",
       "         [-2, -1],\n",
       "         [-2,  0],\n",
       "         [-1, -3],\n",
       "         [-1, -2],\n",
       "         [-1, -1],\n",
       "         [-1,  0],\n",
       "         [ 0, -3],\n",
       "         [ 0, -2],\n",
       "         [ 0, -1],\n",
       "         [ 0,  0]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size=4\n",
    "indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "distances = indices[None, :, :] - indices[:, None, :]\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb62171b-5286-44de-8a04-3f8b4c2ec524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads  # 32 * 3  , 32* 6 ,...\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5 # 1/root(dk)\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement) # to no ruin relative position embedding\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shifted: # regular , shifted \n",
    "            x = self.cyclic_shift(x)\n",
    "\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        nw_h = n_h // self.window_size   # number of windows\n",
    "        nw_w = n_w // self.window_size\n",
    "\n",
    "        q, k, v = map(                 # 10 * 4    10 * 4    3 *32 -> #b #3 #10 * 10    #4*4   # 32\n",
    "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "\n",
    "        if self.shifted:\n",
    "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27091375-fa91-4771-879a-1960ef4e29cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7. Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0a8b0ba-f0b7-42ce-b791-dc78834b091e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1938, -0.0795, -0.7788,  0.0320, -0.4385, -1.0646, -0.9435,  0.5667,\n",
      "         -0.7716, -1.0359]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = SwinTransformer(\n",
    "    hidden_dim=96,\n",
    "    layers=(2, 2, 6, 2),\n",
    "    heads=(3, 6, 12, 24),\n",
    "    channels=3,\n",
    "    num_classes=10,\n",
    "    head_dim=32,\n",
    "    window_size=7,\n",
    "    downscaling_factors=(4, 2, 2, 2),\n",
    "    relative_pos_embedding=True\n",
    ")\n",
    "dummy_x = torch.randn(1, 3, 224, 224)\n",
    "logits = net(dummy_x)  # (1,3)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86257a1c-3e03-49b3-a31a-8cba8589a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models import SwinTransformer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
